# Consecutive Pretraining: A Knowledge Transfer Learning Strategy with Unlabelled Data for Remote Sensing Domain Promotion


### This repository cotains PyTorch implementation and pretrained models of CSPT. 
### Give a star! â­ï¸ if this project helped you.



### Pretrained models:
The pre-trained models based on ViT-B are released in [Model Zoo](https://pan.baidu.com/s/1bhxdjjrVk0jWMs7dnXVQWQ) (code:dspt).




### UpdatesğŸŒŸ :
* May 7, 2022: All pretrained models of various remote sensing downstream tasks are released publicly.
* June 21, 2022: The code about pre-training and fine-tuning is coming soon.


### InstallationğŸš€:
Please refer to [install.md](install.md) for installation.

### Getting StartedğŸš€: 
Please refer to [get_started.md](get_started.md) for the basic usage.

### Acknowledgement
The code is built using the [MAE](https://github.com/facebookresearch/mae), [MMdetection](https://github.com/open-mmlab/mmdetection) and [BEiT](https://github.com/microsoft/unilm/tree/master/beit) repository.

### Citation
```bash
@article{consecutivepretraining2022,
  title={Consecutive Pretraining: A Knowledge Transfer Learning Strategy with Relevant Unlabeled Data For Remote Sensing Domain},
  author={Tong Zhang and Peng Gao and Hao Dong and Yin Zhuang and Guanqun Wang and Wei Zhang and He Chen},
  journal={arXiv preprint arXiv:2207.03860},
  year={2022}
}
```
