# Consecutive Pretraining: A Knowledge Transfer Learning Strategy with Unlabelled Data for Remote Sensing Domain Promotion


### This repository contains PyTorch implementation and pretrained models of CSPT. 
### Give a star! â­ï¸ if this project helped you.



### Pretrained models:
The pre-trained models based on ViT-B are released in [Model Zoo](https://pan.baidu.com/s/1bhxdjjrVk0jWMs7dnXVQWQ) (code:dspt).




### UpdatesğŸŒŸ :
* May 7, 2022: All pretrained models of various remote sensing downstream tasks are released publicly.
* August 1, 2022: Update the code about pre-training and fine-tuning.


### InstallationğŸš€:
Please refer to [install.md](install.md) for installation.

### Getting StartedğŸš€: 
Please refer to [get_started.md](get_started.md) for the basic usage.

### Acknowledgement
The code is built using the [MAE](https://github.com/facebookresearch/mae), [MMdetection](https://github.com/open-mmlab/mmdetection) and [BEiT](https://github.com/microsoft/unilm/tree/master/beit) repository.

### Citation
```bash
@article{zhang2022consecutive,
  title={Consecutive pre-training: A knowledge transfer learning strategy with relevant unlabeled data for remote sensing domain},
  author={Zhang, Tong and Gao, Peng and Dong, Hao and Zhuang, Yin and Wang, Guanqun and Zhang, Wei and Chen, He},
  journal={Remote Sensing},
  volume={14},
  number={22},
  pages={5675},
  year={2022},
  publisher={MDPI}
}
```
